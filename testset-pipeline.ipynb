{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10190214,"sourceType":"datasetVersion","datasetId":6295883},{"sourceId":10193901,"sourceType":"datasetVersion","datasetId":6298645}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"13c1d55a","cell_type":"markdown","source":"# Train setup preprocess part","metadata":{}},{"id":"1d120404-1763-4a83-afb4-66ffdff1106f","cell_type":"markdown","source":"## file -> add input -> add test.csv","metadata":{}},{"id":"c102adfc-15df-4147-b05c-79dae41f4060","cell_type":"markdown","source":"## add-ons -> secrets ->activate HF_TOKEN","metadata":{}},{"id":"1e6193f2-e4db-43c5-af5e-2a6f3e69ddfc","cell_type":"markdown","source":"# LLM prediction part","metadata":{}},{"id":"a47a31ea","cell_type":"markdown","source":"## Installation preparations","metadata":{}},{"id":"b5344d57-b37d-42f0-ad55-5d33cd6ef762","cell_type":"code","source":"# %%capture here\n!curl -LsSf https://astral.sh/uv/install.sh | sh","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:26:25.453485Z","iopub.execute_input":"2024-12-13T22:26:25.453814Z","iopub.status.idle":"2024-12-13T22:26:27.901613Z","shell.execute_reply.started":"2024-12-13T22:26:25.453785Z","shell.execute_reply":"2024-12-13T22:26:27.900352Z"},"trusted":true},"outputs":[{"name":"stdout","text":"downloading uv 0.5.8 x86_64-unknown-linux-gnu\nno checksums to verify\ninstalling to /root/.local/bin\n  uv\n  uvx\neverything's installed!\n\nTo add $HOME/.local/bin to your PATH, either restart your shell or run:\n\n    source $HOME/.local/bin/env (sh, bash, zsh)\n    source $HOME/.local/bin/env.fish (fish)\n","output_type":"stream"}],"execution_count":1},{"id":"257550e0-ccbf-472d-890f-ffab83e5789f","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nimport os\nimport re\nimport json\nos.environ[\"PATH\"] = \"/root/.local/bin:\" + os.environ[\"PATH\"]\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" # magic downloads https://huggingface.co/docs/hub/models-downloading\nos.environ[\"DSP_CACHEBOOL\"] = \"False\" # for quick dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:26:27.903807Z","iopub.execute_input":"2024-12-13T22:26:27.904128Z","iopub.status.idle":"2024-12-13T22:26:28.052477Z","shell.execute_reply.started":"2024-12-13T22:26:27.904098Z","shell.execute_reply":"2024-12-13T22:26:28.051726Z"}},"outputs":[],"execution_count":2},{"id":"c10040d0-f6c5-4fe6-9b9b-03108cc1e5cc","cell_type":"code","source":"%%capture here\n!curl -LsSf https://astral.sh/uv/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:26:28.053428Z","iopub.execute_input":"2024-12-13T22:26:28.053680Z","iopub.status.idle":"2024-12-13T22:26:30.182237Z","shell.execute_reply.started":"2024-12-13T22:26:28.053655Z","shell.execute_reply":"2024-12-13T22:26:30.180751Z"}},"outputs":[],"execution_count":3},{"id":"645a34ff-0189-46a3-a05c-ffd2215e5676","cell_type":"code","source":"%%capture here\n!uv pip install scikit-learn --system","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:26:30.185182Z","iopub.execute_input":"2024-12-13T22:26:30.185665Z","iopub.status.idle":"2024-12-13T22:26:31.787925Z","shell.execute_reply.started":"2024-12-13T22:26:30.185615Z","shell.execute_reply":"2024-12-13T22:26:31.786139Z"}},"outputs":[],"execution_count":4},{"id":"9abc9d1f-9479-49a7-b325-4bb9625e5cb4","cell_type":"code","source":"%%capture here\n!uv pip uninstall torch torchvision torchaudio --system\n!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --system","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:26:31.789819Z","iopub.execute_input":"2024-12-13T22:26:31.790233Z","iopub.status.idle":"2024-12-13T22:27:12.383035Z","shell.execute_reply.started":"2024-12-13T22:26:31.790180Z","shell.execute_reply":"2024-12-13T22:27:12.381232Z"}},"outputs":[],"execution_count":5},{"id":"1fad7ae5-52ee-423a-9568-dc926f86877b","cell_type":"code","source":"%%capture here\n!uv pip install proto-plus==1.24.0.dev1 dataset --system\n!uv pip install --upgrade datasets --system\n!uv pip install vllm --system","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:27:12.385007Z","iopub.execute_input":"2024-12-13T22:27:12.385488Z","iopub.status.idle":"2024-12-13T22:27:31.863676Z","shell.execute_reply.started":"2024-12-13T22:27:12.385440Z","shell.execute_reply":"2024-12-13T22:27:31.862157Z"}},"outputs":[],"execution_count":6},{"id":"6cbd28e5-78cb-49ba-acbc-c6757fff2cad","cell_type":"code","source":"%%capture here\n!uv pip install \"huggingface_hub[hf_transfer]\" --system\n!uv pip install dspy==2.5.34 datasets bitsandbytes triton hf_transfer --system # vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:27:31.865320Z","iopub.execute_input":"2024-12-13T22:27:31.865669Z","iopub.status.idle":"2024-12-13T22:27:36.203003Z","shell.execute_reply.started":"2024-12-13T22:27:31.865637Z","shell.execute_reply":"2024-12-13T22:27:36.201130Z"}},"outputs":[],"execution_count":7},{"id":"592450fb-3328-4b91-b2ff-a7c1810dea51","cell_type":"code","source":"# %%capture here\n!uv pip install ollama --system","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:27:36.205614Z","iopub.execute_input":"2024-12-13T22:27:36.206025Z","iopub.status.idle":"2024-12-13T22:27:37.882321Z","shell.execute_reply.started":"2024-12-13T22:27:36.205981Z","shell.execute_reply":"2024-12-13T22:27:37.881326Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.10.14 environment at: /opt/conda\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 407ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----\u001b[0m\u001b[0m     0 B/12.85 KiB                     \u001b[1A\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m                                                   \u001b[1A\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                  \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.4.4\u001b[0m\n","output_type":"stream"}],"execution_count":8},{"id":"a4aeef2c-95f1-4a83-9ec3-0922b0dcd1f3","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nimport subprocess\nimport dspy\nimport pandas as pd\nimport ollama\nos.environ[\"HF_TOKEN\"] = secret_value_0","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:27:37.883782Z","iopub.execute_input":"2024-12-13T22:27:37.884140Z","iopub.status.idle":"2024-12-13T22:27:46.458138Z","shell.execute_reply.started":"2024-12-13T22:27:37.884110Z","shell.execute_reply":"2024-12-13T22:27:46.457417Z"},"trusted":true},"outputs":[],"execution_count":9},{"id":"cfb05320-1b19-45a7-b252-332a8e28ba2e","cell_type":"code","source":"# %%capture here\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:27:46.461157Z","iopub.execute_input":"2024-12-13T22:27:46.461749Z","iopub.status.idle":"2024-12-13T22:28:41.609740Z","shell.execute_reply.started":"2024-12-13T22:27:46.461719Z","shell.execute_reply":"2024-12-13T22:28:41.608472Z"},"trusted":true},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%                                                             11.3%###########                                      50.5%########################################                        70.5%#####################################                        70.8%########################################                     73.7%######################################################          88.9%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":10},{"id":"05bb396b-a470-43f6-a7a8-db827a6ab199","cell_type":"markdown","source":"# Dataset preparation part","metadata":{}},{"id":"96cb9b04","cell_type":"code","source":"import pandas as pd\nimport json\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.611430Z","iopub.execute_input":"2024-12-13T22:28:41.611790Z","iopub.status.idle":"2024-12-13T22:28:41.616934Z","shell.execute_reply.started":"2024-12-13T22:28:41.611755Z","shell.execute_reply":"2024-12-13T22:28:41.615917Z"}},"outputs":[],"execution_count":11},{"id":"79ee9884-3c9a-4fe4-ab0f-2210e5964041","cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/test-csv/test.csv\",encoding='UTF-8',index_col=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.618055Z","iopub.execute_input":"2024-12-13T22:28:41.618388Z","iopub.status.idle":"2024-12-13T22:28:41.671001Z","shell.execute_reply.started":"2024-12-13T22:28:41.618358Z","shell.execute_reply":"2024-12-13T22:28:41.669830Z"}},"outputs":[],"execution_count":12},{"id":"4db1c1a3","cell_type":"code","source":"STOP_WORDS = {\n    # Prepositions\n     'під', 'над', 'зі', 'біля',\n    'для', 'між', 'перед', 'після', 'без', 'крізь',\n    # Conjunctions\n    'але', 'проте', 'однак', 'чи', 'якщо', 'коли',\n    'щоб', 'бо', 'тому що', 'оскільки',\n    # Articles and particles\n    'же', 'би', 'б', 'хіба', 'невже',\n    #additional\n    'що','...'\n}\ndef decode_entities(json_str):\n    return json.loads(json_str)\n\n\ndef clean_text(text):\n    # Replace all newline characters with a single space\n    cleaned_text = re.sub(r'\\n\\n', ' ', text)\n    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n    # Remove commas and em dashes, replacing them with a single space\n    cleaned_text = re.sub(r'[,—]', ' ', cleaned_text)\n    # Replace consecutive spaces with a single space, iteratively\n    cleaned_text = re.sub(r' {2,}', ' ', cleaned_text)\n    return cleaned_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.672572Z","iopub.execute_input":"2024-12-13T22:28:41.672925Z","iopub.status.idle":"2024-12-13T22:28:41.680313Z","shell.execute_reply.started":"2024-12-13T22:28:41.672892Z","shell.execute_reply":"2024-12-13T22:28:41.679285Z"}},"outputs":[],"execution_count":13},{"id":"b2290926","cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.681461Z","iopub.execute_input":"2024-12-13T22:28:41.681742Z","iopub.status.idle":"2024-12-13T22:28:41.719916Z","shell.execute_reply.started":"2024-12-13T22:28:41.681715Z","shell.execute_reply":"2024-12-13T22:28:41.719041Z"}},"outputs":[],"execution_count":14},{"id":"cac80ad5","cell_type":"code","source":"def split_text_into_sentences(text):\n    # Split text into sentences using regular expressions\n    sentences = re.split(r'(?<=[.!?])\\s*', text)  # Split by period, exclamation, or question mark\n    return [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n\ndef chunk_sentences(sentences, chunk_size, max_length):\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for sentence in sentences:\n        sentence_length = len(sentence)\n        # Check if adding the current sentence exceeds the chunk size or max length\n        if len(current_chunk) < chunk_size and (current_length + sentence_length) <= max_length:\n            current_chunk.append(sentence)\n            current_length += sentence_length\n        else:\n            # Finalize the current chunk and start a new one\n            if current_chunk:\n                chunks.append(current_chunk)\n            current_chunk = [sentence]\n            current_length = sentence_length\n\n    # Add the last chunk if it contains any sentences\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n\ndef process_dataframe(df, chunk_size, max_length):\n    # Apply splitting and chunking to the specified column of the DataFrame\n    df['split_text'] = df['text'].apply(\n        lambda text: chunk_sentences(split_text_into_sentences(text), chunk_size, max_length)\n    )\n    return df\n\ndf = pd.DataFrame(df)\nchunk_size = 6  # Number of sentences per chunk\nmax_length = 600  # Maximum character length per chunk\n\n# Process the DataFrame\ndf = process_dataframe(df, chunk_size, max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.720915Z","iopub.execute_input":"2024-12-13T22:28:41.721185Z","iopub.status.idle":"2024-12-13T22:28:41.754416Z","shell.execute_reply.started":"2024-12-13T22:28:41.721158Z","shell.execute_reply":"2024-12-13T22:28:41.753433Z"}},"outputs":[],"execution_count":15},{"id":"276b0c9d","cell_type":"code","source":"# Flatten the dataframe: each chunk of sentences becomes a new row, rest of columns are duplicated\nexpanded_df = df.explode('split_text', ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.755768Z","iopub.execute_input":"2024-12-13T22:28:41.756166Z","iopub.status.idle":"2024-12-13T22:28:41.768721Z","shell.execute_reply.started":"2024-12-13T22:28:41.756121Z","shell.execute_reply":"2024-12-13T22:28:41.767691Z"}},"outputs":[],"execution_count":16},{"id":"362a333c","cell_type":"code","source":"print(len(expanded_df))\nprint(len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.770042Z","iopub.execute_input":"2024-12-13T22:28:41.770444Z","iopub.status.idle":"2024-12-13T22:28:41.776003Z","shell.execute_reply.started":"2024-12-13T22:28:41.770411Z","shell.execute_reply":"2024-12-13T22:28:41.774961Z"}},"outputs":[{"name":"stdout","text":"1385\n169\n","output_type":"stream"}],"execution_count":17},{"id":"6d117ae7","cell_type":"code","source":"# for i in expanded_df['split_text']:\n#     print(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.777282Z","iopub.execute_input":"2024-12-13T22:28:41.777679Z","iopub.status.idle":"2024-12-13T22:28:41.784022Z","shell.execute_reply.started":"2024-12-13T22:28:41.777633Z","shell.execute_reply":"2024-12-13T22:28:41.783002Z"}},"outputs":[],"execution_count":18},{"id":"bc688829","cell_type":"code","source":"# df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.785572Z","iopub.execute_input":"2024-12-13T22:28:41.786388Z","iopub.status.idle":"2024-12-13T22:28:41.793075Z","shell.execute_reply.started":"2024-12-13T22:28:41.786338Z","shell.execute_reply":"2024-12-13T22:28:41.792338Z"}},"outputs":[],"execution_count":19},{"id":"793c9f32","cell_type":"code","source":"# drop ['']\nexpanded_df = expanded_df[expanded_df['split_text'].apply(lambda x: x != [''])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.794324Z","iopub.execute_input":"2024-12-13T22:28:41.794711Z","iopub.status.idle":"2024-12-13T22:28:41.803717Z","shell.execute_reply.started":"2024-12-13T22:28:41.794663Z","shell.execute_reply":"2024-12-13T22:28:41.802757Z"}},"outputs":[],"execution_count":20},{"id":"593f79b9","cell_type":"code","source":"# tokenize right here\n# Define a function to tokenize and flatten the list of sentences into a single list of tokens\ndef tokenize_text(text_list):\n    # Regex to match words, handling special characters\n    tokenized_list = [re.findall(r'\\S+|\\n', text) for text in text_list]\n    # Flatten the list of lists into a single list\n    return [token for sublist in tokenized_list for token in sublist]\n\n# Apply this function to the 'split_text' column\nexpanded_df['tokenized_text'] = expanded_df['split_text'].apply(tokenize_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.804919Z","iopub.execute_input":"2024-12-13T22:28:41.805597Z","iopub.status.idle":"2024-12-13T22:28:41.872785Z","shell.execute_reply.started":"2024-12-13T22:28:41.805559Z","shell.execute_reply":"2024-12-13T22:28:41.871964Z"}},"outputs":[],"execution_count":21},{"id":"1aad0cc4","cell_type":"code","source":"expanded_df=expanded_df[['id','tokenized_text']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.873870Z","iopub.execute_input":"2024-12-13T22:28:41.874141Z","iopub.status.idle":"2024-12-13T22:28:41.879769Z","shell.execute_reply.started":"2024-12-13T22:28:41.874113Z","shell.execute_reply":"2024-12-13T22:28:41.878746Z"}},"outputs":[],"execution_count":22},{"id":"53654c97","cell_type":"code","source":"# expanded_df.to_csv('test_preprocessed.csv',encoding='UTF-8',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.881052Z","iopub.execute_input":"2024-12-13T22:28:41.881411Z","iopub.status.idle":"2024-12-13T22:28:41.886964Z","shell.execute_reply.started":"2024-12-13T22:28:41.881370Z","shell.execute_reply":"2024-12-13T22:28:41.886063Z"}},"outputs":[],"execution_count":23},{"id":"d0009f51-c73a-4ba3-99ea-0c5dbcd05e54","cell_type":"code","source":"df=expanded_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.888353Z","iopub.execute_input":"2024-12-13T22:28:41.888751Z","iopub.status.idle":"2024-12-13T22:28:41.897191Z","shell.execute_reply.started":"2024-12-13T22:28:41.888721Z","shell.execute_reply":"2024-12-13T22:28:41.896334Z"}},"outputs":[],"execution_count":24},{"id":"1b7615b0","cell_type":"markdown","source":"## Model predict part","metadata":{}},{"id":"dd4782ee-6e61-4709-a259-14f4f92d1397","cell_type":"code","source":"# %%capture here\nprocess = subprocess.Popen(\"ollama serve\", shell=True,) #runs on a different thread #add \"--tensor_parallel_size\", \"2\",","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:28:41.898459Z","iopub.execute_input":"2024-12-13T22:28:41.898770Z","iopub.status.idle":"2024-12-13T22:28:41.909287Z","shell.execute_reply.started":"2024-12-13T22:28:41.898740Z","shell.execute_reply":"2024-12-13T22:28:41.908148Z"},"trusted":true},"outputs":[],"execution_count":25},{"id":"53a1f613-329a-4fdc-9e32-c61fb97b8f97","cell_type":"code","source":"%%capture here\n!ollama pull gemma2:9b-instruct-fp16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:28:41.910741Z","iopub.execute_input":"2024-12-13T22:28:41.911148Z","iopub.status.idle":"2024-12-13T22:31:25.087111Z","shell.execute_reply.started":"2024-12-13T22:28:41.911103Z","shell.execute_reply":"2024-12-13T22:31:25.085825Z"}},"outputs":[{"name":"stdout","text":"Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is: \n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIPWnyXneAvTfuqit3zdU0Chw1TRWf5yk79VxlwWsnMpR\n\n","output_type":"stream"},{"name":"stderr","text":"2024/12/13 22:28:41 routes.go:1195: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2024-12-13T22:28:41.932Z level=INFO source=images.go:753 msg=\"total blobs: 0\"\ntime=2024-12-13T22:28:41.932Z level=INFO source=images.go:760 msg=\"total unused blobs removed: 0\"\ntime=2024-12-13T22:28:41.933Z level=INFO source=routes.go:1246 msg=\"Listening on 127.0.0.1:11434 (version 0.5.1)\"\ntime=2024-12-13T22:28:41.934Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama416840487/runners\ntime=2024-12-13T22:28:42.295Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm]\"\ntime=2024-12-13T22:28:42.295Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\ntime=2024-12-13T22:28:42.657Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-27504d68-cb2c-f518-65df-ecc5be106990 library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2024-12-13T22:28:42.657Z level=INFO source=types.go:123 msg=\"inference compute\" id=GPU-196549d5-7b3d-15f2-4585-502d19112535 library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/12/13 - 22:28:43 | 200 |      47.832µs |       127.0.0.1 | HEAD     \"/\"\n","output_type":"stream"},{"name":"stderr","text":"time=2024-12-13T22:28:43.601Z level=INFO source=download.go:175 msg=\"downloading 76c95736fd14 in 19 1 GB part(s)\"\ntime=2024-12-13T22:30:02.819Z level=INFO source=download.go:175 msg=\"downloading 109037bec39c in 1 136 B part(s)\"\ntime=2024-12-13T22:30:04.238Z level=INFO source=download.go:175 msg=\"downloading 097a36493f71 in 1 8.4 KB part(s)\"\ntime=2024-12-13T22:30:05.425Z level=INFO source=download.go:175 msg=\"downloading 2490e7468436 in 1 65 B part(s)\"\ntime=2024-12-13T22:30:06.623Z level=INFO source=download.go:175 msg=\"downloading 6c29221808fb in 1 486 B part(s)\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/12/13 - 22:31:24 | 200 |         2m41s |       127.0.0.1 | POST     \"/api/pull\"\n","output_type":"stream"}],"execution_count":26},{"id":"8d27c774-3d2b-4ef0-94ac-9ecb7deb96f3","cell_type":"code","source":"import time\ntime.sleep(60*7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T22:31:25.088600Z","iopub.execute_input":"2024-12-13T22:31:25.088934Z","iopub.status.idle":"2024-12-13T22:38:25.152113Z","shell.execute_reply.started":"2024-12-13T22:31:25.088901Z","shell.execute_reply":"2024-12-13T22:38:25.151240Z"}},"outputs":[],"execution_count":27},{"id":"aaad706d-11d0-449a-a16a-c64017520b7d","cell_type":"code","source":"lm = dspy.LM('ollama_chat/gemma2:9b-instruct-fp16', api_base='http://localhost:11434', api_key='')\ndspy.configure(lm=lm)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.156886Z","iopub.execute_input":"2024-12-13T22:38:25.157199Z","iopub.status.idle":"2024-12-13T22:38:25.162686Z","shell.execute_reply.started":"2024-12-13T22:38:25.157169Z","shell.execute_reply":"2024-12-13T22:38:25.161652Z"},"trusted":true},"outputs":[],"execution_count":28},{"id":"f5c56e68-9acf-4e83-bbac-052fdb4d2193","cell_type":"code","source":"# Convert DataFrame rows into dspy.Example objects\ndef create_examples(dataset):\n    examples = [\n        dspy.Example(\n            # id = row[\"id\"],\n            tokens=row[\"tokenized_text\"],\n            #expected_entities=row[\"filtered_entities\"]\n        ).with_inputs(\"tokenized_text\")\n        for _, row in dataset.iterrows()\n    ]\n    return examples\n\n# Example usage with different datasets\ntrain = create_examples(df)\n# test = create_examples(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.163946Z","iopub.execute_input":"2024-12-13T22:38:25.164604Z","iopub.status.idle":"2024-12-13T22:38:25.299532Z","shell.execute_reply.started":"2024-12-13T22:38:25.164559Z","shell.execute_reply":"2024-12-13T22:38:25.298655Z"},"trusted":true},"outputs":[],"execution_count":29},{"id":"dfd9e22f-4be5-482c-9908-398127632579","cell_type":"code","source":"# train","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.300648Z","iopub.execute_input":"2024-12-13T22:38:25.300961Z","iopub.status.idle":"2024-12-13T22:38:25.305326Z","shell.execute_reply.started":"2024-12-13T22:38:25.300932Z","shell.execute_reply":"2024-12-13T22:38:25.304453Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":30},{"id":"0b78506a-a34d-43c7-b4a6-26eac1a784ad","cell_type":"code","source":"class Expert_Linguist(dspy.Signature):\n    \"\"\"EXTRACT ENTITIES:\n    LEAVE THE WORD ENDINGS OR DECLENSIONS OF ANY ENTITY AS IS!!!.\n    JOB: roles/positions\n    PERS: names\n    ORG: organizations\n    LOC: locations\n    ART: artifacts\n    DATE: dates\n    TIME: times\n    PERIOD: durations\n    MONEY: amounts\n    PCT: percentages\n    QUANT: quantities\n    MISC: other named\n    DOC: documents\n    - LEAVE THE WORD ENDINGS OR DECLENSIONS OF ANY ENTITY AS IS!!!. \n      The original form of the entity (with correct case, gender, and declension) should remain intact.\n    - Value for \"JOB\" e.g. can be \"фахівець із IT\"\n    - Value for \"LOC\" e.g. can be \"Чернівців\"\n    - Value for \"ART\" e.g. can be \"Різдвяна історія з Тіною Кароль\"\n    - Value for \"ART\" e.g. can be \"олешківської міськрайонки\"\n    - Value for \"PERS\" e.g. can be \"Іван Підгорський\").\n    - RETURN empty list IF THERE IS NO ENTITIES\"\"\"\n    tokens:list=dspy.InputField()\n    entities =dspy.OutputField(desc=\"List[Dict[str, str]]. like so [{'JOB':'entity entity entity'}, {'ORG':'entity entity'}, {'MONEY':'entity'}] LEAVE THE WORD ENDINGS OR DECLENSIONS OF ANY ENTITY AS IS!!!.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.306465Z","iopub.execute_input":"2024-12-13T22:38:25.306764Z","iopub.status.idle":"2024-12-13T22:38:25.322693Z","shell.execute_reply.started":"2024-12-13T22:38:25.306737Z","shell.execute_reply":"2024-12-13T22:38:25.321767Z"},"trusted":true},"outputs":[],"execution_count":31},{"id":"e81a4bdb-48ac-4e21-b808-b141f1432bfd","cell_type":"code","source":"# len(train)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.323774Z","iopub.execute_input":"2024-12-13T22:38:25.324049Z","iopub.status.idle":"2024-12-13T22:38:25.334612Z","shell.execute_reply.started":"2024-12-13T22:38:25.324023Z","shell.execute_reply":"2024-12-13T22:38:25.333820Z"},"trusted":true},"outputs":[],"execution_count":32},{"id":"a06354fb-5221-4498-862b-48a4ce8fd5c8","cell_type":"code","source":"just_extractor = dspy.ChainOfThought(Expert_Linguist)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.335578Z","iopub.execute_input":"2024-12-13T22:38:25.335855Z","iopub.status.idle":"2024-12-13T22:38:25.348428Z","shell.execute_reply.started":"2024-12-13T22:38:25.335815Z","shell.execute_reply":"2024-12-13T22:38:25.347660Z"},"trusted":true},"outputs":[],"execution_count":33},{"id":"55e9afac-29ef-429b-b49e-2f167f195530","cell_type":"code","source":"%%capture here\nstart = 0       # Початковий індекс\nbatch_size = 3 # # Розмір слайсу 1385 \n\nresult=pd.DataFrame(\n    [\n        {\"id\":row[\"id\"],\n         \"entities\": just_extractor(tokens=dspy.Example(tokens=row[\"tokenized_text\"])).entities\n        } for _, row in df.iloc[start:start+batch_size].iterrows()\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:38:25.349495Z","iopub.execute_input":"2024-12-13T22:38:25.349775Z","iopub.status.idle":"2024-12-13T22:40:22.678663Z","shell.execute_reply.started":"2024-12-13T22:38:25.349748Z","shell.execute_reply":"2024-12-13T22:40:22.676558Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[92m22:38:25 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \nLiteLLM completion() model= gemma2:9b-instruct-fp16; provider = ollama_chat\ntime=2024-12-13T22:38:25.822Z level=INFO source=sched.go:730 msg=\"new model will fit in available VRAM, loading\" model=/root/.ollama/models/blobs/sha256-76c95736fd1483b32c8ad704594349e92fa3ec947c8fea45942caa5bd28df08d library=cuda parallel=4 required=\"24.0 GiB\"\ntime=2024-12-13T22:38:26.055Z level=INFO source=server.go:105 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.7 GiB\" free_swap=\"0 B\"\ntime=2024-12-13T22:38:26.056Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=43 layers.offload=43 layers.split=22,21 memory.available=\"[14.6 GiB 14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"24.0 GiB\" memory.required.partial=\"24.0 GiB\" memory.required.kv=\"2.6 GiB\" memory.required.allocations=\"[12.8 GiB 11.1 GiB]\" memory.weights.total=\"18.1 GiB\" memory.weights.repeating=\"16.4 GiB\" memory.weights.nonrepeating=\"1.7 GiB\" memory.graph.full=\"1.2 GiB\" memory.graph.partial=\"1.2 GiB\"\ntime=2024-12-13T22:38:26.057Z level=INFO source=server.go:397 msg=\"starting llama server\" cmd=\"/tmp/ollama416840487/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-76c95736fd1483b32c8ad704594349e92fa3ec947c8fea45942caa5bd28df08d --ctx-size 8192 --batch-size 512 --n-gpu-layers 43 --threads 2 --parallel 4 --tensor-split 22,21 --port 38449\"\ntime=2024-12-13T22:38:26.059Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2024-12-13T22:38:26.059Z level=INFO source=server.go:576 msg=\"waiting for llama runner to start responding\"\ntime=2024-12-13T22:38:26.060Z level=INFO source=server.go:610 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2024-12-13T22:38:26.630Z level=INFO source=runner.go:941 msg=\"starting go runner\"\ntime=2024-12-13T22:38:26.631Z level=INFO source=runner.go:942 msg=system info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)\" threads=2\ntime=2024-12-13T22:38:26.631Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:38449\"\nllama_model_loader: loaded meta data with 29 key-value pairs and 464 tensors from /root/.ollama/models/blobs/sha256-76c95736fd1483b32c8ad704594349e92fa3ec947c8fea45942caa5bd28df08d (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it\nllama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584\nllama_model_loader: - kv   4:                         gemma2.block_count u32              = 42\nllama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16\nllama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\nllama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\ntime=2024-12-13T22:38:26.816Z level=INFO source=server.go:610 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  169 tensors\nllama_model_loader: - type  f16:  295 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 108\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 3584\nllm_load_print_meta: n_layer          = 42\nllm_load_print_meta: n_head           = 16\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 4096\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 2048\nllm_load_print_meta: n_embd_v_gqa     = 2048\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 9B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 9.24 B\nllm_load_print_meta: model size       = 17.22 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = gemma-2-9b-it\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: EOG token        = 1 '<eos>'\nllm_load_print_meta: EOG token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 93\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n  Device 1: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.62 MiB\nllm_load_tensors: offloading 42 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 43/43 layers to GPU\nllm_load_tensors:        CPU buffer size =  1750.00 MiB\nllm_load_tensors:      CUDA0 buffer size =  8317.20 MiB\nllm_load_tensors:      CUDA1 buffer size =  9311.11 MiB\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1408.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =  1280.00 MiB\nllama_new_context_with_model: KV self size  = 2688.00 MiB, K (f16): 1344.00 MiB, V (f16): 1344.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     3.96 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nllama_new_context_with_model:      CUDA0 compute buffer size =   442.01 MiB\nllama_new_context_with_model:      CUDA1 compute buffer size =   663.02 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =   135.02 MiB\nllama_new_context_with_model: graph nodes  = 1690\nllama_new_context_with_model: graph splits = 3\ntime=2024-12-13T22:39:38.420Z level=INFO source=server.go:615 msg=\"llama runner started in 72.34 seconds\"\nllama_model_loader: loaded meta data with 29 key-value pairs and 464 tensors from /root/.ollama/models/blobs/sha256-76c95736fd1483b32c8ad704594349e92fa3ec947c8fea45942caa5bd28df08d (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it\nllama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584\nllama_model_loader: - kv   4:                         gemma2.block_count u32              = 42\nllama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16\nllama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  11:                          general.file_type u32              = 1\nllama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  169 tensors\nllama_model_loader: - type  f16:  295 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 108\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 1\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = all F32\nllm_load_print_meta: model params     = 9.24 B\nllm_load_print_meta: model size       = 17.22 GiB (16.00 BPW) \nllm_load_print_meta: general.name     = gemma-2-9b-it\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: EOG token        = 1 '<eos>'\nllm_load_print_meta: EOG token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 93\nllama_model_load: vocab only - skipping tensors\n\u001b[92m22:39:50 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n\u001b[92m22:39:50 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \nLiteLLM completion() model= gemma2:9b-instruct-fp16; provider = ollama_chat\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/12/13 - 22:39:50 | 200 |         1m25s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/12/13 - 22:39:50 | 404 |    8.792349ms |       127.0.0.1 | POST     \"/api/show\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[92m22:40:12 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n\u001b[92m22:40:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \nLiteLLM completion() model= gemma2:9b-instruct-fp16; provider = ollama_chat\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/12/13 - 22:40:12 | 200 | 21.588550618s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/12/13 - 22:40:12 | 404 |     123.722µs |       127.0.0.1 | POST     \"/api/show\"\n","output_type":"stream"},{"name":"stderr","text":"\u001b[92m22:40:22 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/12/13 - 22:40:22 | 200 | 10.077518694s |       127.0.0.1 | POST     \"/api/chat\"\n[GIN] 2024/12/13 - 22:40:22 | 404 |     154.641µs |       127.0.0.1 | POST     \"/api/show\"\n","output_type":"stream"}],"execution_count":34},{"id":"6cd39b82-914d-4ea5-af7f-26b0e0432d3b","cell_type":"code","source":"for i in result['entities']:\n    print()\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.680881Z","iopub.execute_input":"2024-12-13T22:40:22.681176Z","iopub.status.idle":"2024-12-13T22:40:22.697298Z","shell.execute_reply.started":"2024-12-13T22:40:22.681147Z","shell.execute_reply":"2024-12-13T22:40:22.696092Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n[{'PERS': 'Тарас Шевченко'}, {'ORG': 'департамент культури і туризму Кіровоградської ОДА'}, {'LOC': 'Кіровоградщини'}, {'LOC': 'Черкащини'}, {'LOC': 'Моринці'}, {'LOC': 'Черкаської області'}]\n\n[{'PERS': 'Тарас Шевченко'}, {'PERS': 'Олександр Кониський'}, {'PERS': 'Катерина Бойко'}]\n\n[{'PERS': 'Тарас Шевченко'}, {'LOC': 'Кирилівка'}, {'LOC': 'Шевченкове'}, {'LOC': 'Звенигородського району'}, {'ORG': 'літературно-меморіальний музей'}]\n","output_type":"stream"}],"execution_count":35},{"id":"13349c87-6e28-40f6-8de4-7fc246ae1a34","cell_type":"code","source":"# # Шлях до існуючого CSV\n# file_path = 'combined.csv'\n\n# # Додавання даних у файл\n# result.to_csv(file_path, mode='a', header=False, index=False)\n\n# print(\"Рядки успішно додано!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.698567Z","iopub.execute_input":"2024-12-13T22:40:22.698867Z","iopub.status.idle":"2024-12-13T22:40:22.707959Z","shell.execute_reply.started":"2024-12-13T22:40:22.698838Z","shell.execute_reply":"2024-12-13T22:40:22.707089Z"},"trusted":true},"outputs":[],"execution_count":36},{"id":"c24791c9-08a8-4838-83d2-df174403fc94","cell_type":"markdown","source":"## Postprocessing of test dataset","metadata":{"execution":{"iopub.execute_input":"2024-12-13T12:20:28.485697Z","iopub.status.busy":"2024-12-13T12:20:28.484833Z","iopub.status.idle":"2024-12-13T12:20:28.489360Z","shell.execute_reply":"2024-12-13T12:20:28.488396Z","shell.execute_reply.started":"2024-12-13T12:20:28.485662Z"}}},{"id":"59c28547-8180-4c40-95e3-03bf0f73e6aa","cell_type":"code","source":"df=pd.read_csv('../input/combined-csv/combined.csv',encoding='UTF-8',index_col=False,)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.709104Z","iopub.execute_input":"2024-12-13T22:40:22.709852Z","iopub.status.idle":"2024-12-13T22:40:22.760203Z","shell.execute_reply.started":"2024-12-13T22:40:22.709808Z","shell.execute_reply":"2024-12-13T22:40:22.759027Z"},"trusted":true},"outputs":[],"execution_count":37},{"id":"43177d6a-0dea-4db9-b4af-b382705ce391","cell_type":"code","source":"# Group by 'id' and concatenate 'filtered_entities'\ndf = df.groupby('id', as_index=False).agg({\n    'filtered_entities': lambda x: ', '.join(x)  # Concatenate strings\n})\n\n# Clean up concatenated string using regex\ndf['filtered_entities'] = df['filtered_entities'].apply(\n    lambda x: re.sub(r'^\\[|\\]$', '', x).replace('][', ', ')\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.761663Z","iopub.execute_input":"2024-12-13T22:40:22.762096Z","iopub.status.idle":"2024-12-13T22:40:22.818189Z","shell.execute_reply.started":"2024-12-13T22:40:22.762046Z","shell.execute_reply":"2024-12-13T22:40:22.817429Z"},"trusted":true},"outputs":[],"execution_count":38},{"id":"9fdc4d8a-8385-45d8-b0d2-af06b745e0b1","cell_type":"code","source":"def convert_to_label_text_format(entity_str):\n    # Regex to match key-value pairs in the new format\n    pattern = r\"\\{\\'([A-Za-z0-9_]+)\\':\\s*\\'([^\\']+)\\'\\}\"\n    \n    # Find all matches\n    matches = re.findall(pattern, entity_str)\n    \n    # Convert each match into a dictionary with 'label' and 'text'\n    converted_entities = [{'label': key, 'text': value} for key, value in matches]\n    \n    # Return the converted entities in string format\n    return str(converted_entities)\n\n# Apply the function to the 'filtered_entities' column\ndf['filtered_entities'] = df['filtered_entities'].apply(convert_to_label_text_format)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.819347Z","iopub.execute_input":"2024-12-13T22:40:22.819657Z","iopub.status.idle":"2024-12-13T22:40:22.832334Z","shell.execute_reply.started":"2024-12-13T22:40:22.819627Z","shell.execute_reply":"2024-12-13T22:40:22.831321Z"},"trusted":true},"outputs":[],"execution_count":39},{"id":"245ca1fc-6c4f-4c46-9697-23bfe1137a91","cell_type":"code","source":"# Function to convert list of dictionaries to JSON string with escaped Unicode characters\ndef convert_to_json(entity_str):\n    # Remove single quotes and handle escape sequences properly\n    entity_str = entity_str.replace(\"'\", \"\\\"\")  # Replace single quotes with double quotes\n    entities = json.loads(entity_str)  # Parse the string to a Python object\n    return json.dumps(entities, ensure_ascii=True)  # Convert to JSON with escaped Unicode\n\n# Apply the function to the 'filtered_entities' column\ndf['filtered_entities'] = df['filtered_entities'].apply(convert_to_json)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.833655Z","iopub.execute_input":"2024-12-13T22:40:22.833925Z","iopub.status.idle":"2024-12-13T22:40:22.850235Z","shell.execute_reply.started":"2024-12-13T22:40:22.833896Z","shell.execute_reply":"2024-12-13T22:40:22.849449Z"},"trusted":true},"outputs":[],"execution_count":40},{"id":"4dcc928a-27bd-439b-aceb-6811d4ca1472","cell_type":"code","source":"df = df.rename(columns={'filtered_entities': 'entities'})\ndf.to_csv('json_out.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.851463Z","iopub.execute_input":"2024-12-13T22:40:22.851744Z","iopub.status.idle":"2024-12-13T22:40:22.890704Z","shell.execute_reply.started":"2024-12-13T22:40:22.851716Z","shell.execute_reply":"2024-12-13T22:40:22.889906Z"},"trusted":true},"outputs":[],"execution_count":41},{"id":"90cd10ea-1c49-4d81-8deb-afda8edce854","cell_type":"code","source":"# Читання CSV файлу\ndf = pd.read_csv('json_out.csv')\n\n# Запис у формат JSONL\ndf.to_json('JSONL.jsonl', orient='records', lines=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T22:40:22.891701Z","iopub.execute_input":"2024-12-13T22:40:22.891983Z","iopub.status.idle":"2024-12-13T22:40:22.910817Z","shell.execute_reply.started":"2024-12-13T22:40:22.891955Z","shell.execute_reply":"2024-12-13T22:40:22.909858Z"},"trusted":true},"outputs":[],"execution_count":42},{"id":"66d73ff0-bffe-49df-bb8c-ce2f19918984","cell_type":"markdown","source":"# THE END","metadata":{"execution":{"iopub.status.busy":"2024-12-13T21:41:26.212542Z","iopub.execute_input":"2024-12-13T21:41:26.212895Z","iopub.status.idle":"2024-12-13T21:41:26.216953Z","shell.execute_reply.started":"2024-12-13T21:41:26.212865Z","shell.execute_reply":"2024-12-13T21:41:26.215956Z"}}}]}